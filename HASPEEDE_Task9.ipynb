{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIHTO6216m4J9s4K3tkl1D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alessandro-vecchi/HASPEEDE/blob/main/HASPEEDE_Task9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HASPEEDE\n"
      ],
      "metadata": {
        "id": "dqkOgAH6AdvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HASPEEDE is a task about identification of hateful content online."
      ],
      "metadata": {
        "id": "TWS7SCn4xv81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Imports ###\n",
        "import numpy as np, pandas as pd, random, re, html, json\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "OTbXktdOAZRZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "wJ-_9QaQIsMS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyMjF2gw2dHJ",
        "outputId": "2478d0ae-5a68-4153-ee33-fc53f7054527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "LPEw_IclSN2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are dealing with a binary classification problem where the goal is to identify whether a given text contains hate speech or is neutral. The dataset is structured as JSON lines, each containing a text string, possible classification choices, and a label indicating the correct class."
      ],
      "metadata": {
        "id": "qGepIfRKyAC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = Path(\"drive/MyDrive/HASPEEDE/train-taskA.jsonl\")\n",
        "test_news_path = Path(\"drive/MyDrive/HASPEEDE/test-news-taskA.jsonl\")\n",
        "test_tweets_path = Path(\"drive/MyDrive/HASPEEDE/test-tweets-taskA.jsonl\")"
      ],
      "metadata": {
        "id": "eS_BuW09AOOa"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = Path(\"https://drive.google.com/file/d/1l_oZOOc7ZzzJvK8VLiLvLzN1dKJz2OyB/view?usp=drive_link\")\n",
        "test_news_path = Path(\"https://drive.google.com/file/d/1BSL5xW0VgR6XZUVcrAF3l5K3zCtoh_O8/view?usp=drive_link\")\n",
        "test_tweets_path = Path(\"https://drive.google.com/file/d/1UnM_JRwqx1jvNh2bJdg_zPyORQ5GhWeX/view?usp=drive_link\")"
      ],
      "metadata": {
        "id": "H39jwrZTQq3d"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head drive/MyDrive/HASPEEDE/train-taskA.jsonl -n 15 # -n NUM print the first NUM lines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXh9ffiAByCd",
        "outputId": "c73917c6-b686-4b8c-b803-e23fb9304b60"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\": \"\\u00c8 terrorismo anche questo, per mettere in uno stato di soggezione le persone e renderle innocue, mentre qualcuno... URL \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"@user @user infatti finch\\u00e9 ci hanno guadagnato con i campi #rom tutto era ok con #Alemanno #Ipocriti \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Corriere: Tangenti, Mafia Capitale dimenticataMazzette su buche e campi rom URL #roma \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"@user ad uno ad uno, perch\\u00e9 quando i migranti israeliti arrivarono in terra di Canaan fecero fuori tutti i Canaaniti. \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Il divertimento del giorno? Trovare i patrioti italiani che inneggiano contro i rom facendo la spesa alla #Lidl (multinazionale tedesca). \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Modena: Comune paga la benzina ai nomadi che portano figli a scuola: MODENA \\u2013 La giunta PD\\u2026 URL \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"@user @user altro che islam o cristianesimo!!! ...a c dobbiamo sorbire anche \\\"dell'ignorante\\\" : islam \\u00e8  uno solo!! \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"@user @user @user grazie stef stavo giusto caricando ho anche messo Che Salvini avallava il finanziare campi rom \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"... e smettetela di dire che anche gli italiani sono stati migranti, erano trattati male ma non per questo uccidevano innocenti per strada. \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Minorenne rom arrestato dopo furto in appartamento URL \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Studentessa cinese morta fermato un nomade di 20anni e denunciato nomade sedicenne tutti del vicino campo nomade SkyTG24 \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Ddl di Calderoli: \\u201cIl velo islamico e l\\u2019apologia della Sharia diventino reato\\u201d. Siete\\u00a0d\\u2019accordo? URL \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Roma, rapina al campo rom La Barbuta: tre arresti. L'ombra del racket URL URL \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Rischiando di essere presa per pazza che non sa di cosa parla#Trump dopo i musulmani e i gay.quelli di colore?#benvenutoHitler?#preoccupante \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n",
            "{\"text\": \"Povert\\u00e0, esclusione e scarsa istruzione: \\\"4100 minori rom hanno una vita segnata alla nascita\\\" URL \", \"choices\": [\"neutrale\", \"odio\"], \"label\": 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonl_to_df(filepath):\n",
        "    data = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Load training data\n",
        "train_df = load_jsonl_to_df(train_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "cYtm-muNhlvW",
        "outputId": "da785a75-c114-4419-b09f-7fdab8fa4872"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https:/drive.google.com/file/d/1l_oZOOc7ZzzJvK8VLiLvLzN1dKJz2OyB/view?usp=drive_link'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-8a1275b66ef8>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_jsonl_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-74-8a1275b66ef8>\u001b[0m in \u001b[0;36mload_jsonl_to_df\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_jsonl_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https:/drive.google.com/file/d/1l_oZOOc7ZzzJvK8VLiLvLzN1dKJz2OyB/view?usp=drive_link'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Data"
      ],
      "metadata": {
        "id": "iU_5vgbtmCPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding data is a crucial step for several reasons:\n",
        "\n",
        "- **Model Design**: Data insights inform algorithm selection, preprocessing, and feature engineering.\n",
        "- **Accuracy Improvement**: Detailed data knowledge allows precise model tuning to improve accuracy.\n",
        "- **Bias Identification**: Early data analysis detects biases, ensuring fairness and ethical model use.\n",
        "- **Training Efficiency**: Proper understanding optimizes training by setting correct validation, class balancing, and managing fitting issues.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tsi6K750oAFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, let's see how big are the datasets we are getting. Knowing the number of entries in each dataset segment (training, testing) helps plan how to split data for training and validation, ensures there's enough data for robust testing, and sets expectations for model evaluation. It also indicates the volume of data the model will handle, which influences decisions on computational resources and training time."
      ],
      "metadata": {
        "id": "7m7aXSIqnp0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Size\n",
        "\n",
        "- **Training and Validation Split**: Knowledge of dataset sizes helps in effectively splitting data for training and validation.\n",
        "- **Robust Testing**: Ensures there is sufficient data for reliable model testing and evaluation.\n",
        "- **Resource Allocation**: Informs the required computational resources and expected training durations."
      ],
      "metadata": {
        "id": "GzRcVl8Eqq2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_lines(path: Path) -> int:\n",
        "    with open(path, \"rb\") as f:\n",
        "        return len(f.readlines())\n",
        "\n",
        "print(f\"Total number of lines in training dataset: {count_lines(train_path)}\")\n",
        "print(f\"Total number of lines in the test_news dataset: {count_lines(test_news_path)}\")\n",
        "print(f\"Total number of lines in test_tweets dataset: {count_lines(test_tweets_path)}\")"
      ],
      "metadata": {
        "id": "n4D1Xw7bmGwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Analysis**:\n",
        "- The training set size (6,839 entries) is large enough to train a model, but it's probably better to consider validation split strategies to avoid overfitting.\n",
        "- The sizes of the test datasets (500 for news and 1,263 for tweets) are adequate for testing but highlight the need for careful model evaluation to ensure generalizability across different types of content."
      ],
      "metadata": {
        "id": "9odvVEzNrvVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class distribution in training data"
      ],
      "metadata": {
        "id": "j_zACaoysMnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This distribution shows a relatively balanced dataset, with a slight skew towards 'neutrale' classes.\n",
        "\n",
        "A simple baseline model could predict the majority class ('neutrale'), therefore **60%** is the minimum benchmark to surpass, ensuring that improvements are due to learning and not random chance.\n",
        "\n",
        "**Reminders**:\n",
        "\n",
        "The slight imbalance highlights the importance of the model performing well on the 'odio' class to avoid biases. A model overly biased towards predicting 'neutrale' could miss critical instances of 'odio', undermining its practical utility.\n",
        "Techniques such as class weighting or oversampling might be employed to address the slight imbalance and increare the accuracy."
      ],
      "metadata": {
        "id": "yU9zXS6nuNvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze class distribution\n",
        "class_distribution = train_df['label'].value_counts(normalize=True).reset_index()  # normalized to show percentages\n",
        "\n",
        "class_distribution.columns = ['Sentiment', 'Percentage']\n",
        "\n",
        "class_distribution['Sentiment'] = ['Neutrale', 'Odio']\n",
        "class_distribution"
      ],
      "metadata": {
        "id": "cWslwvs-l_Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.bar(class_distribution['Sentiment'], class_distribution['Percentage'], color='skyblue')\n",
        "plt.xlabel('Sentiment', fontsize=14)\n",
        "plt.ylabel('Percentages %', fontsize=14)\n",
        "plt.title('Class Distribution in the Dataset', fontsize=16)\n",
        "# plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()  # Adjust layout to not cut off labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pQykjfscsdZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze text lengths\n",
        "train_df['text_length_words'] = train_df['text'].apply(lambda x: len(x.split()))\n",
        "train_df['text_length_chars'] = train_df['text'].apply(len)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "pUtuzvcuiiti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics for text lengths in words\n",
        "average_length_words = train_df['text_length_words'].mean()\n",
        "max_length_words = train_df['text_length_words'].max()\n",
        "\n",
        "print(f\"Average Length in Words: {average_length_words}\")\n",
        "print(f\"Maximum Length in Words: {max_length_words}\")"
      ],
      "metadata": {
        "id": "essMdVWrilKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram of text lengths in words\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(train_df['text_length_words'], bins=30)\n",
        "plt.title('Distribution of Text Lengths by Words')\n",
        "plt.xlabel('Text Length (Words)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "81LVZltfkpnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics for text lengths in characters\n",
        "average_length_chars = train_df['text_length_chars'].mean()\n",
        "max_length_chars = train_df['text_length_chars'].max()\n",
        "print(f\"Average Length in Characters: {average_length_chars}\")\n",
        "print(f\"Maximum Length in Characters: {max_length_chars}\")"
      ],
      "metadata": {
        "id": "j-SS_ZNxkdX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram of text lengths in characters\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(train_df['text_length_chars'], bins=30)\n",
        "plt.title('Distribution of Text Lengths by Characters')\n",
        "plt.xlabel('Text Length (Characters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cUA8NF-6knnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary analysis\n",
        "all_words = [word.lower() for text in train_df['text'] for word in text.split()]\n",
        "word_counts = Counter(all_words)\n",
        "vocabulary_size = len(word_counts)\n",
        "most_common_words = word_counts.most_common(20)\n",
        "\n",
        "print(f\"Vocabulary Size: {vocabulary_size}\")\n",
        "print(\"Most Common Words:\")\n",
        "for word, freq in most_common_words:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "id": "mknhorFsiy9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all text data into one large string\n",
        "text = ' '.join(train_df['text'])\n",
        "\n",
        "# Create a word cloud object\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color ='white',\n",
        "                          max_words=200, contour_width=3, contour_color='steelblue').generate(text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.title('Word Cloud of Most Common Words in Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p0tDafmcju1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_val(df, props=[.8, .2]):\n",
        "    assert round(sum(props), 2) == 1 and len(props) == 2\n",
        "    train_df, val_df = None, None\n",
        "\n",
        "    size1 = int(props[0]*len(df))\n",
        "    train_df = df.iloc[: size1]\n",
        "    val_df = df.iloc[size1:]\n",
        "\n",
        "    return train_df, val_df"
      ],
      "metadata": {
        "id": "QuNg7LVDLIWM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}